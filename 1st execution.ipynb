{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c12fa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\heman\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Lambda\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0969626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf161ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cd54693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path =\"ano.xlsx\"\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13de47a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Disease</th>\n",
       "      <th>Age</th>\n",
       "      <th>Symptoms_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Influenza (Flu)</td>\n",
       "      <td>30</td>\n",
       "      <td>I woke up suddenly with a high fever, around ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dengue</td>\n",
       "      <td>28</td>\n",
       "      <td>The fever hit me like a wave, reaching 104°F....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Malaria</td>\n",
       "      <td>32</td>\n",
       "      <td>I've been feeling unwell for the past few day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Typhoid</td>\n",
       "      <td>27</td>\n",
       "      <td>I've been having a prolonged fever, around 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Common Cold</td>\n",
       "      <td>25</td>\n",
       "      <td>I caught a cold, and it started with sneezing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Disease  Age                               Symptoms_Description\n",
       "0   Influenza (Flu)      30   I woke up suddenly with a high fever, around ...\n",
       "1   Dengue               28   The fever hit me like a wave, reaching 104°F....\n",
       "2   Malaria              32   I've been feeling unwell for the past few day...\n",
       "3   Typhoid              27   I've been having a prolonged fever, around 10...\n",
       "4   Common Cold          25   I caught a cold, and it started with sneezing..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "034bab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Disease'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7fed23f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b3a6fd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Influenza (Flu)', 'Dengue', 'Malaria', 'Typhoid', 'Common Cold'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Disease'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "897049cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a5878578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'by', 'what', 'the', \"she's\", 'won', 'my', 'no', 'his', 'off', 'wouldn', 'not', 'that', 'about', \"didn't\", 'in', 'him', 'd', 've', 'during', \"you'd\", 'am', 'until', 'she', 'yourself', 'further', 's', 'doing', 'its', 'themselves', 'out', 't', 'should', 'such', 'each', 'aren', 'into', 'ain', \"won't\", \"weren't\", 'can', 'was', 'most', 'here', 'very', 'himself', 'her', 'itself', 'couldn', 'haven', 'has', 'they', 'i', 'this', 'up', 'ours', \"you'll\", 're', \"isn't\", 'to', 'don', \"mightn't\", 'yourselves', 'any', 'own', 'he', 'shan', 'why', 'at', \"couldn't\", \"needn't\", 'do', 'for', 'are', 'both', 'whom', \"hadn't\", 'or', \"hasn't\", 'before', 'as', 'mightn', 'm', \"that'll\", 'just', 'same', 'which', 'our', 'be', 'does', 'mustn', 'few', 'isn', \"aren't\", 'their', 'o', 'more', 'after', 'so', 'other', 'did', \"it's\", 'theirs', 'y', 'you', 'them', 'above', 'too', \"wouldn't\", 'hasn', 'an', 'over', \"shouldn't\", \"shan't\", 'some', \"you've\", 'down', 'there', 'how', 'had', 'when', 'if', 'hers', 'where', 'from', 'under', 'being', 'these', 'doesn', 'once', \"should've\", \"doesn't\", 'yours', 'than', 'me', 'needn', 'having', 'is', 'below', 'ma', 'weren', \"you're\", 'through', 'didn', 'who', 'but', 'while', 'between', 'again', 'ourselves', 'against', 'with', 'hadn', 'nor', 'then', 'of', 'only', 'been', 'we', 'were', 'on', 'those', 'shouldn', 'myself', 'your', 'it', 'herself', 'have', 'and', 'wasn', \"haven't\", 'will', 'because', \"wasn't\", 'll', 'now', 'a', \"don't\", 'all', \"mustn't\"}\n"
     ]
    }
   ],
   "source": [
    "#nltk stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1c182d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy stopwords\n",
    "#import spacy\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "#stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "#print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f6f6a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d6f3009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Disease', 'Age', 'Symptoms_Description'], dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c2fd5ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'] = df['Age'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "19597067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Disease', 'Age', 'Symptoms_Description'], dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f58bf87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_Symptoms'] = df['Symptoms_Description'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b5c1036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['Encoded_Disease'] = label_encoder.fit_transform(df['Disease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "19866c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['Cleaned_Symptoms'], df['Encoded_Disease'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cb717ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'caught cold started sneezing runny nose throat feel scratchy sore cough thats bothering there mucus dripping throat also feel bit feverish day symptom improving much'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cleaned_Symptoms'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dc935da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Padding\n",
    "max_words = 1000  # Adjust based on your data\n",
    "max_length = 20  # Adjust based on your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "75ee56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "501abf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "432bb10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_vector_features = 40\n",
    "# temperature_parameter = 0.8  # Adjust based on your preference\n",
    "\n",
    "# input_text = Input(shape=(max_length,))\n",
    "# embedding_layer = Embedding(max_words, embedding_vector_features, input_length=max_length)(input_text)\n",
    "# lstm_layer = LSTM(100)(embedding_layer)\n",
    "# dropout_layer = Dropout(0.3)(lstm_layer)\n",
    "\n",
    "# scaled_temperature = Lambda(lambda x: x / temperature_parameter)\n",
    "\n",
    "# concatenated_layer = tf.keras.layers.concatenate([dropout_layer, scaled_temperature])\n",
    "\n",
    "# dense_layer = Dense(df['Encoded_Disease'].nunique(), activation='softmax')(concatenated_layer)\n",
    "\n",
    "# model = Model(inputs=[input_text], outputs=dense_layer)\n",
    "\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "95893e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 20, 40)            40000     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               56400     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 505       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 96905 (378.54 KB)\n",
      "Trainable params: 96905 (378.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_features = 40\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_vector_features, input_length=max_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(df['Encoded_Disease'].nunique(), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "83fb761b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 2s 260ms/step - loss: 1.6071 - accuracy: 0.2250 - val_loss: 1.5986 - val_accuracy: 0.4146\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 1.5926 - accuracy: 0.5500 - val_loss: 1.5851 - val_accuracy: 0.7073\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 1.5779 - accuracy: 0.6625 - val_loss: 1.5677 - val_accuracy: 0.7317\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 1.5546 - accuracy: 0.6938 - val_loss: 1.5427 - val_accuracy: 0.7317\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 1.5204 - accuracy: 0.6812 - val_loss: 1.5017 - val_accuracy: 0.6585\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 1.4623 - accuracy: 0.6562 - val_loss: 1.4310 - val_accuracy: 0.3902\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 1.3515 - accuracy: 0.5562 - val_loss: 1.2924 - val_accuracy: 0.5122\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 1.1334 - accuracy: 0.6375 - val_loss: 1.0592 - val_accuracy: 0.8049\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.9215 - accuracy: 0.8438 - val_loss: 0.8329 - val_accuracy: 0.8049\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.7253 - accuracy: 0.8687 - val_loss: 0.6904 - val_accuracy: 0.7317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1c0ce7ba190>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_padded, y_train, validation_data=(X_test_padded, y_test), epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ae1db060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6904 - accuracy: 0.7317\n",
      "Test Loss: 0.6903509497642517\n",
      "Test Accuracy: 73.17%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dc277bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['Disease'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3a79d36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[[]]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Prediction 1: Typhoid, Probability: 0.4038892686367035\n",
      "Prediction 2: Common Cold, Probability: 0.19351652264595032\n",
      "Prediction 3: Dengue, Probability: 0.17699331045150757\n",
      "Prediction 4: Malaria, Probability: 0.12420020997524261\n",
      "Prediction 5: Influenza (Flu), Probability: 0.10140068084001541\n"
     ]
    }
   ],
   "source": [
    "mxl=20\n",
    "new_symptom = input()\n",
    "new_symptom = preprocess_text(new_symptom)\n",
    "print(new_symptom)\n",
    "new_symptom_sequence = tokenizer.texts_to_sequences([new_symptom])\n",
    "print(new_symptom_sequence)\n",
    "new_symptom_padded = pad_sequences(new_symptom_sequence, maxlen=mxl, truncating='post')\n",
    "print(new_symptom_padded)\n",
    "\n",
    "predictions = model.predict([new_symptom_padded])[0]\n",
    "\n",
    "# Get the top N predictions along with their probabilities\n",
    "top_n = len(df['Disease'].unique())  # You can adjust this based on how many top predictions you want\n",
    "top_indices = tf.math.top_k(predictions, k=top_n).indices.numpy()\n",
    "top_probabilities = tf.math.top_k(predictions, k=top_n).values.numpy()\n",
    "\n",
    "# Decode the label indices to actual disease labels\n",
    "top_diseases = label_encoder.inverse_transform(top_indices)\n",
    "\n",
    "# Print the results\n",
    "for i in range(top_n):\n",
    "    print(f\"Prediction {i + 1}: {top_diseases[i]}, Probability: {top_probabilities[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "30604cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Influenza (Flu)', 'Dengue', 'Malaria', 'Typhoid', 'Common Cold'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Disease'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b59179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
